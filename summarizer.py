from typing import List
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_cohere import ChatCohere
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os
import logging
import tiktoken
load_dotenv()
print("API:", os.environ.get("COHERE_API_KEY"))


logging.basicConfig(level=logging.INFO)
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² __name__ ÛŒÚ© Ø±ÙˆØ´ Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ logger Ø§Ø³Øª.
logger = logging.getLogger(__name__) 

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†"""
    encoding = tiktoken.get_encoding(encoding_name)
    return len(encoding.encode(string))

class AdvancedSummarizer:
    def __init__(self):
        self.llm = None
        self.initialize_llm()

    def initialize_llm(self):
        """Initialize the language model"""
        api_key = os.environ.get("COHERE_API_KEY")
        if not api_key:
            raise ValueError("âš ï¸ COHERE_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª (Ø¯Ø± Hugging Face Secrets Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯).")

        self.llm = ChatCohere(
    cohere_api_key=os.environ["COHERE_API_KEY"],
    model="command-a-03-2025",
    temperature=0.1,
    max_tokens=5000,
)


    def smart_text_split(self, text: str) -> List[Document]:
        """
        ØªÙ‚Ø³ÛŒÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ù…ØªÙ† Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ 
        Ø§Ø² Ù¾Ù†Ø¬Ø±Ù‡ Ø²Ù…ÛŒÙ†Ù‡ (Command-A).
        """
        # ðŸ’¡ Ø¨Ù‡Ø¨ÙˆØ¯: Ø§ÙØ²Ø§ÛŒØ´ chunk_size ØªØ§ 35000 ØªÙˆÚ©Ù†. 
        # Ø§ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø³Øª Ùˆ Ø§Ø² Ù¾Ù†Ø¬Ø±Ù‡ Ø²Ù…ÛŒÙ†Ù‡ Ù…Ø¯Ù„ 128k Ø¨Ù‡ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
    length_function=num_tokens_from_string,
    separators=["\n\n", "\n", " ", ""]
)

        return [Document(page_content=chunk) for chunk in splitter.split_text(text)]

    def summarize_large_text(self, text: str, language: str = "persian") -> str:
        """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            if not text or not text.strip():
                return "âŒ Ù‡ÛŒÚ† Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."
            
            total_tokens = num_tokens_from_string(text)
            logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§: {total_tokens}")
            
            # ðŸ’¡ Ø¨Ù‡Ø¨ÙˆØ¯: Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ 40,000 ØªÙˆÚ©Ù†
            if total_tokens > 40000:  
                return "âš ï¸ Ù…ØªÙ† Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ (Ø­Ø¯Ø§Ú©Ø«Ø± 30,000 ØªÙˆÚ©Ù†)."
            
            # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ†
            docs = self.smart_text_split(text)
            logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø®Ø´â€ŒÙ‡Ø§: {len(docs)}")
            
            if len(docs) == 1:
                # Ø§Ú¯Ø± Ù…ØªÙ† Ø¯Ø± ÛŒÚ© Ø¨Ø®Ø´ Ø¬Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯
                return self.summarize_directly(docs[0].page_content, language)
            else:
                # Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø§Ø² map_reduce Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
                return self.summarize_map_reduce(docs, language)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ: {str(e)}")
            return f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {str(e)}"

    def summarize_directly(self, text: str, language: str) -> str:
        """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©"""
        prompt = ChatPromptTemplate.from_template(
            "Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† {language} Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯. Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒØŒ "
            "Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§Ø´Ø¯:\n\n{text}"
        )
        
        chain = prompt | self.llm
        result = chain.invoke({"text": text, "language": language})
        return result.content

    def summarize_map_reduce(self, docs: List[Document], language: str) -> str:
        """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ map-reduce Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯"""
        map_prompt_template = ChatPromptTemplate.from_template(
            "Ø¨Ø®Ø´ Ø²ÛŒØ± Ø§Ø² ÛŒÚ© Ù…ØªÙ† Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† {language} Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯. "
            "Ø±ÙˆÛŒ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… ØªÙ…Ø±Ú©Ø² Ú©Ù†ÛŒØ¯:\n\n{text}"
        )

        combine_prompt_template = ChatPromptTemplate.from_template(
            "Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ÛŒÚ© Ù…ØªÙ† Ù‡Ø³ØªÙ†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ù…Ù†Ø³Ø¬Ù… "
            "Ùˆ Ø¬Ø§Ù…Ø¹ Ø¨Ù‡ Ø²Ø¨Ø§Ù† {language} ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯. Ø³Ø§Ø®ØªØ§Ø± Ù…Ù†Ø·Ù‚ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ùˆ "
            "Ù‡Ù…Ù‡ Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡Ø¯:\n\n{text}"
        )

        chain = load_summarize_chain(
            self.llm,
            chain_type="map_reduce",
            map_prompt=map_prompt_template,
            combine_prompt=combine_prompt_template,
            verbose=True
        )

        result = chain.invoke({
            "input_documents": docs, 
            "language": language
        })
        
        return result["output_text"]

# Ù†Ù…ÙˆÙ†Ù‡ Ø§ØµÙ„ÛŒ
summarizer = AdvancedSummarizer()
def summarize_text(text: str, language: str = "persian") -> str:
    """ØªØ§Ø¨Ø¹ wrapper Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ"""
    return summarizer.summarize_large_text(text, language)